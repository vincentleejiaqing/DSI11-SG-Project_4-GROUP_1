{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dummies for species\n",
    "train1=pd.get_dummies(train,columns=['Species'],drop_first=True)    \n",
    "train1['Date']=pd.to_datetime(train1['Date'])\n",
    "\n",
    "\n",
    "\n",
    "# Let's create a function that finds the days since the most recent spray, \n",
    "# given a date of collection minus dates of spraying:\n",
    "def recent(delMin,delMax):\n",
    "    if (delMin>=0) & (delMax<0):\n",
    "        dell=delMin\n",
    "    elif (delMax>=0) & (delMin<0):\n",
    "        dell=delMax\n",
    "    elif (delMax<0) & (delMin<0):\n",
    "        dell=3650\n",
    "    elif (delMax>=0) & (delMin>=0):\n",
    "        if delMax<delMin:\n",
    "            dell=delMax\n",
    "        elif delMax>delMin:\n",
    "            dell=delMin\n",
    "        else:\n",
    "            dell=delMin\n",
    "    elif delMin.isnull() or delMax.isnull():\n",
    "        dell=3650\n",
    "    return(dell)\n",
    "#def engineerSP(spray):\n",
    " \n",
    "\n",
    "spray1=spray.copy()\n",
    "# because geo-location has such high resolution, we can't find matches between the data \n",
    "#sets (which doesn't allow us to merge the tables accordingly). so 3 digits after the \n",
    "# point seems reasonable - equates to 100 meters in resolution ( according to quick \n",
    "# exploration on google maps: .001 ~=100 Meters)\n",
    "spray1['Longitude']=spray['Longitude'].round(3) ## rounding to match longitudes in train data (resolution of 100 M)\n",
    "spray1['Latitude']=spray['Latitude'].round(3)\n",
    "train1['Longitude']=train1['Longitude'].round(3)\n",
    "train1['Latitude']=train1['Latitude'].round(3)\n",
    "spray1['Date']=pd.to_datetime(spray1['Date']) ## change to datetime object so operations could be done \n",
    "spray2=spray1.groupby(['Longitude','Latitude']).Date.agg(['count','min','max',np.ptp]).reset_index() ## get the range of spraying for each site\n",
    "spray2=spray2.sort_values('ptp',ascending=False)\n",
    "#merge train with spray:\n",
    "sptrainright=pd.merge(spray2,train1,on=['Longitude','Latitude'],how='right',indicator=True) ## merge train with spray \n",
    "sptrainright['Date']=pd.to_datetime(sptrainright['Date']) \n",
    "sptrainright['delmin']=sptrainright['Date']-sptrainright['min']\n",
    "sptrainright['delmin']=sptrainright['delmin'].dt.days\n",
    "sptrainright['delmax']=sptrainright['Date']-sptrainright['max']\n",
    "sptrainright['delmax']=sptrainright['delmax'].dt.days\n",
    "sptrainright['most_recent_spray_(days)']=3650\n",
    "## previously to create a column with the most recent spray in days\n",
    "# after 962 obdservations with spray, the rest is without data on spray\n",
    "# Make an inner merge to concentrate on the spray&train intersection (and avoid dealing with NaT and NaN:\n",
    "sptraininner=pd.merge(spray2,train1,on=['Longitude','Latitude'],how='inner',indicator=True)    \n",
    "sptraininner['Date']=pd.to_datetime(sptraininner['Date'])\n",
    "sptraininner['delmin']=sptraininner['Date']-sptraininner['min']\n",
    "sptraininner['delmin']=sptraininner['delmin'].dt.days\n",
    "sptraininner['delmax']=sptraininner['Date']-sptraininner['max']\n",
    "sptraininner['delmax']=sptraininner['delmax'].dt.days\n",
    "sptraininner['most_recent_spray_(days)']=sptraininner.apply(lambda x: recent(x['delmin'],x['delmax']),axis=1)\n",
    "# Now conocat the dfs\n",
    "sptrain=pd.concat([sptraininner,sptrainright.loc[963:,:]])\n",
    "# arrange columns name\n",
    "'''\n",
    "colls=sptrain.columns\n",
    "colis=list(colls)\n",
    "colsnew=colis[:2]+colis[3:5]+[colis[-1]]+colis[6:14]+colis[15:21]+colis[14:15] \n",
    "sptrain=sptrain[colsnew]\n",
    "'''\n",
    "sptrain.rename(columns={'Date':'Date_of_collection'},inplace=True)\n",
    "sptrain.drop(['_merge'],1,inplace=True)\n",
    "# let' add a column - whether the area was recently sprayed (i.e. <150 days)\n",
    "sptrain['Recently_sprayed']=(sptrain['most_recent_spray_(days)']<150).astype(int)\n",
    "sptrain['Recently_sprayed'].value_counts()\n",
    "\n",
    "'''##########################################################'''\n",
    "\n",
    "''' WEATHER PART - CLEAN, ENGINEER AND MERGE \n",
    "    (on: 'day of collection'=='day of forecast) '''\n",
    "\n",
    "'''##########################################################'''\n",
    "\n",
    "weather_csv=os.path.join(directory_path,\"all/weather.csv\")\n",
    "weather=pd.read_csv(weather_csv)\n",
    "\n",
    "## Create weather station column in the train data (2 different stations: 1 and 2)\n",
    "## first create a function to get classify which weather station fits which trap collection (by geo-location)\n",
    "# assumption: long and lat have the same resolution when converted to distance.\n",
    "# for distance calculation using: distance^2=long^2+lat^2\n",
    "\n",
    "# Reminder the geo-location of the stations: \n",
    "# Station 1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level\n",
    "# Station 2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level\n",
    "import random\n",
    "def stationing(long,lat):\n",
    "    sta1={'long':-87.933,'lat': 41.995}\n",
    "    sta2={'long':-87.752,'lat': 41.786}\n",
    "    longdelt1=long-sta1['long']\n",
    "    latdelt1=lat-sta1['lat']\n",
    "    dist1 = np.sqrt((longdelt1)**2+(latdelt1)**2)\n",
    "    #print(\"dist1 =\",dist1)\n",
    "    longdelt2=long-sta2['long']\n",
    "    latdelt2=lat-sta2['lat']\n",
    "    dist2 = np.sqrt((longdelt2)**2+(latdelt2)**2)\n",
    "    #print(\"dist2 =\",dist2)\n",
    "    if dist1<dist2:\n",
    "        #print(\"stat1\",dist1)\n",
    "        station=1\n",
    "    elif dist1>dist2:\n",
    "        station=2\n",
    "        #print(\"stat2\",dist2)\n",
    "    else:\n",
    "        station=random.choice([1,2])\n",
    "    return(station)\n",
    "    \n",
    "sptrainW=sptrain.copy() # make a copy of sptrainW so we can add weather (W) features to it. (starting with station column) \n",
    "sptrainW['station']=sptrainW.apply(lambda x: stationing(x.Longitude,x.Latitude),axis=1)  \n",
    "## using stationing function to match station number to every observation\n",
    "## now we have station column. ~80% of the traps are closer to station 2\n",
    "weather['Date']=pd.to_datetime(weather['Date'])\n",
    "\n",
    "# merging weather and sptrain data \n",
    "sptrainW0=pd.merge(sptrainW,weather,left_on=['Date_of_collection','station'],right_on=['Date','Station'],how='left',indicator=True)\n",
    "## sptrainW0 is the merged dataset before any engineering and cleanning of it\n",
    "sptrainW1=sptrainW0.copy()\n",
    "## let's start engineering:\n",
    "sptrainW1.drop(['station','Date'],1,inplace=True)\n",
    "sptrainW1.drop(['_merge',],1,inplace=True)\n",
    "sptrainW1.drop(['Depart',],1,inplace=True)# Depart - # Most is M missing (8223), so droping that column\n",
    "#Since number of missing values are small (26), mode or median would make sense as replacement. \n",
    "# But after doing quick search online, we can approximate WetBulb from DewPoint and \n",
    "# temperature (that we have) with this formula - TAVG-((TAVG-DEWPOINT)/3).\n",
    "# (resource: http://theweatherprediction.com/habyhints/170/ )\n",
    "# Writing a function for wetbulb approximation:\n",
    "def wetbulb(tavg,dp,wb):\n",
    "    if wb=='M':\n",
    "        wb=tavg-(tavg-dp)/3\n",
    "    else: \n",
    "        pass\n",
    "    return(wb)\n",
    "\n",
    "\n",
    "sptrainW1['Tavg']=sptrainW1['Tavg'].astype(int) ## numbers are stored as str - so turn to int, to manipulate \n",
    "# applying it to the df:  \n",
    "sptrainW1['WetBulb']=sptrainW1.apply(lambda x: wetbulb(x['Tavg'],x['DewPoint'],x['WetBulb']),axis=1)\n",
    "sptrainW1['WetBulb']=sptrainW1['WetBulb'].astype(int)\n",
    "## continuing switching str into int in other columns\n",
    "sptrainW1['Heat']=sptrainW1['Heat'].astype(int)\n",
    "sptrainW1['Cool']=sptrainW1['Cool'].astype(int)\n",
    "sptrainW1.drop(['Sunrise','Sunset'],1,inplace=True) ## mostly empty\n",
    "#Function to turn codes into 2 groups good (' ') and bad weather ( all other codes)\n",
    "# the assumption is that all codes are related to bad weather..\n",
    "def codes(col):\n",
    "    if col==' ':\n",
    "        col='Norm'\n",
    "    else:\n",
    "        col='Bad'\n",
    "    return(col)\n",
    "codes(' ')\n",
    "\n",
    "sptrainW1['weather_type']=sptrainW1.apply(lambda x: codes(x['CodeSum']), axis=1)\n",
    "sptrainW1=pd.get_dummies(sptrainW1,columns=['weather_type'],drop_first=True)\n",
    "# to check: type sptrainW1.weather_type_Norm.value_counts()\n",
    "sptrainW1.drop(['Water1','SnowFall'],1,inplace=True) ## see summary, mostly missing values\n",
    "# PrecipTotal - convert T (trace) to 0.005 (look at summary):\n",
    "sptrainW1['PrecipTotal']=sptrainW1['PrecipTotal'].apply(lambda x: 0.005 if x=='  T' else x)\n",
    "#convert 'M' to mode \n",
    "import statistics as st\n",
    "mode=st.mode(sptrainW1['PrecipTotal']) # mode is '0'\n",
    "sptrainW1['PrecipTotal']=sptrainW1['PrecipTotal'].apply(lambda x: mode if x=='M' else x)\n",
    "sptrainW1['PrecipTotal']=sptrainW1['PrecipTotal'].astype(float) # converting to type float.\n",
    " # Depth:\n",
    "sptrainW1.drop('Depth',inplace=True,axis=1) # drop, mostly 'M' rest 0 (see summary)\n",
    "# Stn Pressure:\n",
    "moud=st.mode(sptrainW1['StnPressure'])\n",
    "sptrainW1['StnPressure']=sptrainW1['StnPressure'].apply(lambda x: moud if x=='M' else x)\n",
    "sptrainW1['StnPressure']=sptrainW1['StnPressure'].astype(float)\n",
    "# Sealevel\n",
    "sptrainW1['SeaLevel']=sptrainW1['SeaLevel'].astype(float)\n",
    "## 'ResultSpeed', 'ResultDir' are good to go (floats no missing value)\n",
    "sptrainW1['AvgSpeed']=sptrainW1['AvgSpeed'].astype(float) # turn to float\n",
    "# date of colletion\n",
    "# let's split the date to day of the month, day of the week, month, year the strongest \n",
    "# effect would probably be the month (where the hottest months will have highest frequency of WNV), \n",
    "# and maybe recent years have been hotter, maybe traps collected on Monday have more mosquitos in them? etc.\n",
    "\n",
    "sptrainW1['Day_of_month']=sptrainW1['Date_of_collection'].apply(lambda x: x.to_pydatetime().day)\n",
    "sptrainW1['month']=sptrainW1['Date_of_collection'].apply(lambda x: x.to_pydatetime().month)\n",
    "sptrainW1['year']=sptrainW1['Date_of_collection'].apply(lambda x: x.to_pydatetime().year)\n",
    "sptrainW1['Day_of_week']=sptrainW1['Date_of_collection'].apply(lambda x: x.to_pydatetime().weekday())\n",
    "sptrainW1['year']=sptrainW1['year']-(min(sptrainW1['year'])+1)\n",
    "\n",
    "## df almost ready. Let's arange and drop columns:\n",
    "sptrainW1.drop(['count','Block','min', 'max','ptp','Address',\n",
    "                'Street','Trap','AddressNumberAndStreet','CodeSum','delmin', 'delmax'],axis=1,inplace=True)\n",
    "\n",
    "## most recent spray (days)\" feature is really effecting the feature space (because of the \n",
    " ## majority of fabricated 3500 days). let's turn this column into 3 categories: \n",
    " ## recently sprayed (this season <180 days), sprayed 2 yrs ago, and never sprayed (=3650).\n",
    "mid=sptrainW1['most_recent_spray_(days)'][(\n",
    "        sptrainW1['most_recent_spray_(days)']<3650)&(sptrainW1['most_recent_spray_(days)']>180)]\n",
    "sptrainW1['sprayed_2_yrs_ago']=sptrainW1['most_recent_spray_(days)'].apply(lambda x: 1 if sum(x==mid)>0 else 0)\n",
    "sptrainW1['never_sprayed']=sptrainW1['most_recent_spray_(days)'].apply(lambda x: 1 if x>3640 else 0)    \n",
    "    \n",
    "sptrainW2=sptrainW1.copy()\n",
    "sptrainW2=sptrainW2[['Longitude', 'Latitude', 'Date_of_collection', 'AddressAccuracy',\n",
    "       'NumMosquitos', 'Species_CULEX PIPIENS',\n",
    "       'Species_CULEX PIPIENS/RESTUANS', 'Species_CULEX RESTUANS',\n",
    "       'Species_CULEX SALINARIUS', 'Species_CULEX TARSALIS',\n",
    "       'Species_CULEX TERRITANS', 'most_recent_spray_(days)',\n",
    "       'Recently_sprayed', 'Station', 'Tmax', 'Tmin', 'Tavg', 'DewPoint',\n",
    "       'WetBulb', 'Heat', 'Cool', 'PrecipTotal', 'StnPressure', 'SeaLevel',\n",
    "       'ResultSpeed', 'ResultDir', 'AvgSpeed', 'weather_type_Norm',\n",
    "       'Day_of_month', 'month', 'year', 'Day_of_week', 'sprayed_2_yrs_ago',\n",
    "       'never_sprayed', 'WnvPresent']]\n",
    "\n",
    "'''###################################\n",
    "Weather - 2 weeks feature engineering \n",
    "#####################################\" '''\n",
    "\n",
    "\n",
    "# Functions to use inside weath_eng func:\n",
    "#1) getting ag temp when there is 'M' - missing data\n",
    "def tavg_fix(tavg_col,max_col,min_col):\n",
    "   # Mind=np.where(W1['Tavg']=='M')\n",
    "    if tavg_col=='M':\n",
    "        tavg_col=(max_col+min_col)/2\n",
    "    else: \n",
    "        pass\n",
    "    return(tavg_col)\n",
    "    \n",
    "#2)\n",
    "    # Wetbulb:\n",
    "#Since number of missing values are small (26), mode or median would make sense as replacement. \n",
    "# But after doing quick search online, we can approximate WetBulb from DewPoint and \n",
    "# temperature (that we have) with this formula - TAVG-((TAVG-DEWPOINT)/3).\n",
    "# Writing a function for wetbulb approximation:\n",
    "def wetbulb(tavg,dp,wb):\n",
    "    if wb=='M':\n",
    "        wb=tavg-(tavg-dp)/3\n",
    "    else: \n",
    "        pass\n",
    "    return(wb)\n",
    "    \n",
    "#3)\n",
    "import statistics as st\n",
    "\n",
    "def M_rid(col,num,thing='M'):\n",
    "    if col==thing:\n",
    "        col=num\n",
    "    else: \n",
    "        pass\n",
    "    return(col)\n",
    "\n",
    "#4)\n",
    "#Function to turn codes into 2 groups good (' ') and bad weather ( all other codes)\n",
    "def codes(col):\n",
    "    if col==' ':\n",
    "        col='Norm'\n",
    "    else:\n",
    "        col='Bad'\n",
    "    return(col)\n",
    "codes(' ')\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "def weath_eng(weather_raw):\n",
    "###\n",
    "\n",
    "    W10=weather_raw.copy()\n",
    "    \n",
    "    W10.drop(['Depart',],1,inplace=True)# Depart - # Most is M missing (8223), so droping that column\n",
    "    \n",
    "         \n",
    "    W10['Tavg']=W10.apply(lambda x: tavg_fix(x['Tavg'],x['Tmax'],x['Tmin']),axis=1)\n",
    "    \n",
    "    W10['Tavg']=W10['Tavg'].astype(float) ## numbers are stored as str - so turn to float, to manipulate \n",
    "      \n",
    "    # applying it to the df:  \n",
    "    W10['WetBulb']=W10.apply(lambda x: wetbulb(x['Tavg'],x['DewPoint'],x['WetBulb']),axis=1)\n",
    "    W10['WetBulb']=W10['WetBulb'].astype(float)\n",
    "    \n",
    "        \n",
    "    W10['Heat']=W10.apply(lambda x: M_rid(x['Heat'],'0'),axis=1)\n",
    "    ## continuing switching str into int in other columns\n",
    "    W10['Heat']=W10['Heat'].astype(int)\n",
    "    \n",
    "    W10['Cool']=W10.apply(lambda x: M_rid(x['Cool'],' 0'),axis=1)\n",
    "    \n",
    "    W10['Cool']=W10['Cool'].astype(int)\n",
    "    \n",
    "    W10.drop(['Sunrise','Sunset'],1,inplace=True) ## mostly empty\n",
    "    \n",
    "    W10['weather_type']=W10.apply(lambda x: codes(x['CodeSum']), axis=1)\n",
    "\n",
    "    W10=pd.get_dummies(W10,columns=['weather_type'],drop_first=True)\n",
    "    \n",
    "    # to check: type sptrainW1.weather_type_Norm.value_counts()\n",
    "    W10.drop(['Water1','SnowFall'],1,inplace=True) ## see summary, mostly missing values\n",
    "    # PrecipTotal - convert T (trace) to 0.005 (look at summary):\n",
    "    W10['PrecipTotal']=W10['PrecipTotal'].apply(lambda x: 0.005 if x=='  T' else x)\n",
    "    #convert 'M' to mode \n",
    "    mode=st.mode(W10['PrecipTotal']) # mode is '0'\n",
    "    W10['PrecipTotal']=W10['PrecipTotal'].apply(lambda x: mode if x=='M' else x)\n",
    "    W10['PrecipTotal']=W10['PrecipTotal'].astype(float) # converting to type float.\n",
    "     # Depth:\n",
    "    W10.drop('Depth',inplace=True,axis=1) # drop, mostly 'M' rest 0 (see summary)\n",
    "    # Stn Pressure:\n",
    "    moud=st.mode(W10['StnPressure'])\n",
    "    W10['StnPressure']=W10['StnPressure'].apply(lambda x: moud if x=='M' else x)\n",
    "    W10['StnPressure']=W10['StnPressure'].astype(float)\n",
    "    # Sealevel\n",
    "    mod=st.mode(W10['SeaLevel'])\n",
    "    W10['SeaLevel']=W10['SeaLevel'].apply(lambda x: mod if x=='M' else x)\n",
    "    W10['SeaLevel']=W10['SeaLevel'].astype(float)\n",
    "\n",
    "    ## 'ResultSpeed', 'ResultDir' are good to go (floats no missing value)\n",
    "    mod=st.mode(W10['AvgSpeed'])\n",
    "    W10['AvgSpeed']=W10['AvgSpeed'].apply(lambda x: mod if x=='M' else x)\n",
    "    W10['AvgSpeed']=W10['AvgSpeed'].astype(float) # turn to float\n",
    "    \n",
    "    return(W10)\n",
    "\n",
    "weath_out=weath_eng(weather)\n",
    "\n",
    "\n",
    "'''#####################################\n",
    "Functions to engineer weather data into summaries (mean,std etc..) of \n",
    "last 14 days for every trap collection (observation) \n",
    " \n",
    "########################################'''\n",
    "\n",
    "# functions to make 14 day summary \n",
    "# summary of one day\n",
    "## important - column need to be clean, and dtype: int or float\n",
    "import scipy.stats as scist\n",
    "\n",
    "def get_summary(colum):\n",
    "    des=colum.describe()  # use describe to get summary\n",
    "    des=pd.DataFrame(des)  # turn into data frame\n",
    "    desT=des.T  # transpose describe to a table \n",
    "    desT.rename(columns={'mean':desT.index[0]+'.'+'mean','std':desT.index[0]+'.'+'std',\n",
    "                    '50%':desT.index[0]+'.'+'50%'},inplace=True) ## rename columns\n",
    "    desT.drop(['count','min','25%','75%','max'],1,inplace=True)\n",
    "    desT[desT.index[0]+'.'+'mean-median']=desT[desT.index[0]+'.'+'mean']-desT[desT.index[0]+'.'+'50%'] # add mean-median\n",
    "    outliers_low=sum((scist.zscore(colum)<-2))\n",
    "    outliers_high=sum((scist.zscore(colum)>2))\n",
    "    desT[desT.index[0]+'.'+'outliers_low']=outliers_low ## add outliers\n",
    "    desT[desT.index[0]+'.'+'outliers_high']=outliers_high\n",
    "    desT.reset_index(drop=True,inplace=True)\n",
    "    return desT\n",
    "\n",
    "## using summary of one day to run on 14 days and make a summary:\n",
    "def fourteen(col_date,station,cols,num_days=14):   # get the 14 day batch of a collection date\n",
    "    # col_date is one Date of collection \n",
    "    # list of columns we'd like to use for the summary of the batch \n",
    "    ind=np.where((col_date==weath_out['Date'])\n",
    "             &(station==weath_out['Station']))\n",
    "    indt=int(ind[0])\n",
    "    datee=weath_out.loc[indt,'Date']\n",
    "    datee=pd.Series(datee)\n",
    "    stat=pd.Series(station)\n",
    "    dd=weath_out.iloc[(indt-(num_days*2)):indt,]\n",
    "    ddd=dd[dd['Station']==station] # the 14 day (default) batch\n",
    "    ss=pd.DataFrame()\n",
    "    ss=pd.concat([ss,datee,stat],axis=1)\n",
    "    for col in cols:\n",
    "        summ=get_summary(ddd[col])\n",
    "        ss=pd.concat([ss,summ],axis=1)\n",
    "#     print(ss)\n",
    "#     print('shape',ss.shape)\n",
    "#     print('type',type(ss))\n",
    "#     print('####')\n",
    "    return(ss)\n",
    "\n",
    "## script to create the engineered features for 14 days:\n",
    "\n",
    "int_feat=['Tmax', 'Tmin', 'Tavg', 'DewPoint', \n",
    "          'WetBulb','Heat', 'Cool', 'PrecipTotal', 'StnPressure',\n",
    "          'ResultSpeed','ResultDir', 'AvgSpeed', 'weather_type_Norm']\n",
    "# tried to do it with apply and lambda, but there seems to be a bug. have to use for loop to run on the whole data set\n",
    "# which will be very slow.\n",
    "# this is how it should be done with apply:\n",
    "#  rrr.iloc[34:46]=sptrainW2.iloc[0:3,:].apply(lambda x: fourteen(x['Date_of_collection'],x['Station'],\n",
    "#                        ['ResultSpeed','Tmax']),axis=1)\n",
    "import time\n",
    "\n",
    "def make_features(clean_weath,feat_of_interest):   ## input clean weather data, and list of feature names to engineer\n",
    "    tt=time.time()\n",
    "    sso=pd.DataFrame()\n",
    "    for i in range(clean_weath.shape[0]):\n",
    "        row=fourteen(sptrainW2.loc[i,'Date_of_collection'],sptrainW2.loc[i,'Station'],feat_of_interest)\n",
    "        sso=pd.concat([sso,row],axis=0)\n",
    "        #if i==12:\n",
    "         #   break\n",
    "    sso.drop(['weather_type_Norm.std','weather_type_Norm.50%','weather_type_Norm.mean-median',\n",
    "              'weather_type_Norm.outliers_low','weather_type_Norm.outliers_high'],1,inplace=True)\n",
    "    sso.rename(columns={0:'Date_colect',1:'station'},inplace=True)\n",
    "    ttt=time.time()\n",
    "    print('time of running:',ttt-tt)\n",
    "    print('new data shape: ',sso.shape)\n",
    "    return(sso)\n",
    "\n",
    "''' Let's run the function and get the engineered weather data '''\n",
    "## warnning: this could take a while (approx 241 seconds)\n",
    "## turn statement into True to run the function\n",
    "if 1==0:\n",
    "    eng_weath=make_features(weath_out,int_feat) \n",
    "\n",
    "#repository path: directory_path\n",
    "if 1==0:\n",
    "    eng_weath.to_csv(os.path.join(directory_path,'eng_weath.csv'))\n",
    "    eng_weath=pd.read_csv(\"/Users/eran/Galvanize_more_repositories/WestNileVirus/eng_weath.csv\")\n",
    "#eng_weath.shape=(2944, 75)\n",
    "#sptrainW2.shape=(10506, 35)\n",
    "\n",
    "# merging spray&weather df (with weather on the day of) with engneered weather\n",
    "sptrain_engW=pd.merge(sptrainW2,eng_weath,left_on=['Date_of_collection','Station'],right_on=['Date_colect','station'],indicator=True)\n",
    "## Getting a massive df because of duplicates. getting rid of them:\n",
    "# sptrain_engW.shape=(293598, 111)\n",
    "yep=sptrain_engW.drop_duplicates()\n",
    "#yep.shape=(9296, 111)\n",
    "\n",
    "## DROP THE \"OF THE DAY\" WEATHER DATA FEATURES.\n",
    "yep.drop(['Tmax', 'Tmin', 'Tavg', 'DewPoint',\n",
    "       'WetBulb', 'Heat', 'Cool', 'PrecipTotal', 'StnPressure', 'SeaLevel',\n",
    "       'ResultSpeed', 'ResultDir', 'AvgSpeed', 'weather_type_Norm'],1,inplace=True)\n",
    "\n",
    "yep.drop(['Date_colect','Station','_merge'],1,inplace=True)\n",
    "\n",
    "yep.drop(['Date_of_collection'],1,inplace=True)\n",
    "\n",
    "#saving the data sets:\n",
    "# sptrainW_day_of.csv - spray+train+weather data (the day of collection)\n",
    "# sptrainW_14_days.csv - spray+train+weather data (summary of weather of 14 days before collection)\n",
    "if 1==0:\n",
    "    sptrainW2.drop(['Date_of_collection','Station'],1,inplace=True)\n",
    "    sptrainW2.to_csv(os.path.join(directory_path,'sptrainW_day_of.csv'))\n",
    "    yep.to_csv(os.path.join(directory_path,'sptrainW_14_days.csv'))\n",
    "    trainBasic=train1[['Latitude','Longitude','AddressAccuracy','NumMosquitos','Species_CULEX PIPIENS','Species_CULEX PIPIENS/RESTUANS','Species_CULEX RESTUANS','Species_CULEX SALINARIUS','Species_CULEX TARSALIS','Species_CULEX TERRITANS','WnvPresent']]\n",
    "    trainBasic.to_csv(os.path.join(directory_path,'train_baseline.csv'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
